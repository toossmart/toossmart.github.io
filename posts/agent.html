<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <link rel="icon" href="/assets/avatar.jpg" type="image/jpeg">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>agent开发 - Toossssmart</title>
    <meta name="description" content="LangChain基础概念 LangChain是什么？ 是开发LLM相关业务的集大成者，是Python的第三方库，提供了各种API LangChain的功能（有哪些API）？ Prompts——提示词工程 Models——各类模型 History——管理会话历史记录 Indexes——分析管理各类文档 Chain——链式执行 Agent——构建智能体 现有LLM存在的问题： LLM不是实时的，不具...">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            /* === 黑白极简色调 === */

            /* 主色调：纯黑 */
            --primary-color: #000000;
            --primary-hover: #333333;
            /* 选中状态/背景色：极淡的灰色 */
            --primary-light: rgba(0, 0, 0, 0.05);

            /* 文本颜色：中性灰阶 */
            --text-primary: #111111;  /* 接近纯黑 */
            --text-secondary: #555555; /* 深灰 */
            --text-tertiary: #999999;  /* 浅灰 */

            /* 背景颜色：纯白与极浅灰 */
            --bg-primary: #ffffff;
            --bg-secondary: #fafafa;  /* 极其轻微的灰色，用于区分区块 */
            --bg-tertiary: #f4f4f4;

            /* 边框和阴影：更锐利或更轻微 */
            --border-color: #e5e5e5;
            --border-light: #f0f0f0;

            /* 阴影保持原样，或者可以变得更淡一点 */
            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
            --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
            --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.05);
            --shadow-hover: 0 20px 25px -5px rgba(0, 0, 0, 0.1);

            /* 代码块：保持深色模式，或者你可以改成浅灰色背景 */
            --code-bg: #1e1e1e;       /* 纯黑代码背景 */
            --code-text: #e0e0e0;

            /* 字体变量保持不变 */
            --font-sans: -apple-system, BlinkMacSystemFont, "Segoe UI", "Inter", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif;
            --font-mono: "SFMono-Regular", "Consolas", "Liberation Mono", "Menlo", "JetBrains Mono", monospace;
        }

        body {
            font-family: var(--font-sans);
            background-color: var(--bg-secondary);
            color: var(--text-primary);
            line-height: 1.75;
            font-size: 15px;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        a {
            color: inherit;
            text-decoration: none;
            transition: color 0.2s;
        }

        .navbar {
            background: var(--bg-primary);
            border-bottom: 1px solid var(--border-color);
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            z-index: 100;
            height: 64px;
        }

        .nav-container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 24px;
            height: 100%;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .nav-logo {
            font-family: var(--font-mono);
            font-size: 20px;
            font-weight: 600;
            color: var(--primary-color);
            letter-spacing: -0.5px;
        }

        .nav-logo a {
            color: inherit;
        }

        .nav-menu {
            display: flex;
            gap: 32px;
        }

        .nav-menu a {
            color: var(--text-secondary);
            font-size: 14px;
            font-weight: 500;
            padding: 4px 0;
        }

        .nav-menu a:hover,
        .nav-menu a.active {
            color: var(--primary-color);
        }

        .main-layout {
            max-width: 1400px;
            margin: 64px auto 0;
            padding: 40px 24px;
            display: grid;
            grid-template-columns: 280px 1fr;
            gap: 40px;
            min-height: calc(100vh - 64px);
        }

        .sidebar {
            position: sticky;
            top: 104px;
            height: fit-content;
        }

        .toc-card {
            background: var(--bg-primary);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 24px;
            box-shadow: var(--shadow-sm);
        }

        .toc-title {
            font-size: 14px;
            font-weight: 600;
            color: var(--text-tertiary);
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 16px;
        }

        .toc-list {
            list-style: none;
        }

        .toc-item {
            margin-bottom: 8px;
        }

        .toc-link {
            display: block;
            font-size: 13px;
            color: var(--text-secondary);
            padding: 6px 8px;
            border-radius: 6px;
            transition: all 0.2s;
            line-height: 1.5;
        }

        .toc-link:hover {
            background: var(--bg-secondary);
            color: var(--primary-color);
        }

        .toc-link.active {
            background: var(--primary-light);
            color: var(--primary-color);
            font-weight: 500;
        }

        .toc-level-2 {
            padding-left: 8px;
        }

        .toc-level-3 {
            padding-left: 20px;
        }

        .toc-level-4 {
            padding-left: 32px;
        }

        .main-content {
            min-width: 0;
        }

        .article-detail {
            background: var(--bg-primary);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 40px;
            box-shadow: var(--shadow-sm);
        }

        .article-title {
            font-size: 28px;
            font-weight: 700;
            margin-bottom: 20px;
            line-height: 1.3;
            letter-spacing: -0.5px;
            color: var(--text-primary);
        }

        .article-meta {
            display: flex;
            gap: 24px;
            color: var(--text-secondary);
            font-size: 14px;
            margin-bottom: 32px;
            padding-bottom: 24px;
            border-bottom: 1px solid var(--border-color);
            font-family: var(--font-mono);
        }

        .article-content {
            font-size: 16px;
            line-height: 1.8;
            color: var(--text-primary);
        }

        .markdown-body {
            font-family: var(--font-sans);
        }

        .markdown-body h1,
        .markdown-body h2,
        .markdown-body h3,
        .markdown-body h4,
        .markdown-body h5,
        .markdown-body h6 {
            margin-top: 32px;
            margin-bottom: 16px;
            font-weight: 700;
            line-height: 1.3;
            letter-spacing: -0.3px;
            scroll-margin-top: 80px;
        }

        .markdown-body h1 {
            font-size: 28px;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border-color);
        }

        .markdown-body h2 {
            font-size: 24px;
        }

        .markdown-body h3 {
            font-size: 20px;
        }

        .markdown-body p {
            margin-bottom: 20px;
        }

        .markdown-body code {
            background: var(--bg-tertiary);
            padding: 2px 8px;
            border-radius: 4px;
            font-family: var(--font-mono);
            font-size: 0.9em;
            color: var(--primary-color);
        }

        .markdown-body pre {
            background: var(--code-bg);
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 24px 0;
            border: 1px solid var(--border-color);
        }

        .markdown-body pre code {
            background: transparent;
            padding: 0;
            color: var(--code-text);
            font-size: 13px;
            line-height: 1.6;
        }

        .markdown-body blockquote {
            border-left: 3px solid var(--primary-color);
            padding-left: 16px;
            margin: 24px 0;
            color: var(--text-secondary);
            font-style: italic;
            background: var(--bg-secondary);
            padding: 16px 16px 16px 20px;
            border-radius: 0 8px 8px 0;
        }

        .markdown-body ul,
        .markdown-body ol {
            margin-bottom: 24px;
            padding-left: 24px;
        }

        .markdown-body li {
            margin-bottom: 8px;
            line-height: 1.7;
        }

        .markdown-body a {
            color: var(--primary-color);
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s;
        }

        .markdown-body a:hover {
            border-bottom-color: var(--primary-color);
        }

        .markdown-body img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 24px 0;
            box-shadow: var(--shadow-md);
            display: block;
        }

        .markdown-body table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            font-size: 14px;
        }

        .markdown-body th,
        .markdown-body td {
            padding: 12px 16px;
            text-align: left;
            border: 1px solid var(--border-color);
        }

        .markdown-body th {
            background: var(--bg-secondary);
            font-weight: 600;
            color: var(--text-primary);
        }

        .markdown-body tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        .footer {
            text-align: center;
            padding: 32px 24px;
            color: var(--text-tertiary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            background: var(--bg-primary);
        }

        .footer a {
            color: var(--primary-color) !important;
            border-bottom-color: var(--primary-color) !important;
        }

        @media (max-width: 1024px) {
            .main-layout {
                grid-template-columns: 1fr;
                gap: 32px;
            }

            .sidebar {
                position: static;
            }
        }

        @media (max-width: 768px) {
            .nav-container {
                padding: 0 16px;
            }

            .main-layout {
                padding: 32px 16px;
            }

            .article-detail {
                padding: 24px;
            }

            .article-title {
                font-size: 22px;
            }
        }

        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }

        ::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-tertiary);
        }

        ::selection {
            background: var(--primary-light);
            color: var(--text-primary);
        }
    </style>
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <a href="index.html">Toossssmart</a>
            </div>
            <div class="nav-menu">
                <a href="index.html" class="active">Blog</a>
                <a href="about.html" class="">About</a>
            </div>
        </div>
    </nav>

    <div class="main-layout">
        <aside class="sidebar">
            <div class="toc-card">
                <div class="toc-title">Table of Contents</div>
                <ul class="toc-list">
                    
                    <li class="toc-item toc-level-2">
                        <a href="#langchain" class="toc-link">LangChain基础概念</a>
                    </li>
                    
                    <li class="toc-item toc-level-2">
                        <a href="#langchain_1" class="toc-link">LangChain三种模型代码示例</a>
                    </li>
                    
                    <li class="toc-item toc-level-2">
                        <a href="#prompt" class="toc-link">prompt优化</a>
                    </li>
                    
                    <li class="toc-item toc-level-2">
                        <a href="#chain" class="toc-link">Chain的使用</a>
                    </li>
                    
                    <li class="toc-item toc-level-2">
                        <a href="#document-loaders" class="toc-link">Document loaders文档加载</a>
                    </li>
                    
                    <li class="toc-item toc-level-2">
                        <a href="#vector-stores" class="toc-link">Vector stores向量存储</a>
                    </li>
                    
                    <li class="toc-item toc-level-2">
                        <a href="#_1" class="toc-link">基于向量检索构建提示词</a>
                    </li>
                    
                </ul>
            </div>
        </aside>

        <main class="main-content">
            
<article class="article-detail">
    <h1 class="article-title">agent开发</h1>

    <div class="article-meta">
        <span>2026-02-5</span>
        
        <span>1 min read</span>
    </div>

    <div class="article-content markdown-body">
        <h2 id="langchain">LangChain基础概念</h2>
<ul>
<li>
<p>LangChain是什么？</p>
</li>
<li>
<p>是开发LLM相关业务的集大成者，是Python的第三方库，提供了各种API</p>
</li>
<li>
<p>LangChain的功能（有哪些API）？</p>
</li>
<li>
<p>Prompts——提示词工程</p>
</li>
<li>Models——各类模型</li>
<li>History——管理会话历史记录</li>
<li>Indexes——分析管理各类文档</li>
<li>Chain——链式执行</li>
<li>
<p>Agent——构建智能体</p>
</li>
<li>
<p>现有LLM存在的问题：</p>
</li>
<li>
<p>LLM不是实时的，不具备更新知识的能力</p>
</li>
<li>专业知识领域知识缺乏，无法覆盖特定专业领域内部知识</li>
<li>幻觉问题</li>
<li>
<p>数据安全</p>
</li>
<li>
<p>RAG是什么？</p>
</li>
</ul>
<p><img alt="image-20260204153525857" src="/assets/image-20260204153525857.png" /></p>
<ul>
<li>RAG工作分为两条线：<ul>
<li>在线流程：用户提问</li>
<li>离线流程：知识库预处理</li>
</ul>
</li>
</ul>
<p><img alt="image-20260204154639558" src="/assets/image-20260204154639558.png" /></p>
<ul>
<li>
<p>通过余弦相似度计算向量是否匹配</p>
</li>
<li>
<p>LangChain的三种模型：</p>
</li>
<li>
<p>LLMs：大参数、海量文本的Transformer架构模型，用于理解生成自然语言。服务于文本生成场景</p>
</li>
<li>聊天模型：Chat Models，专为聊天对话优化的LLMs</li>
<li>文本嵌入模型：Embedding Models，文本作为输入，得到文本向量</li>
</ul>
<h2 id="langchain_1">LangChain三种模型代码示例</h2>
<ul>
<li>LLMs</li>
</ul>
<pre><code class="language-python">from langchian_community.llms.tongyi import Tongyi

model=Tongyi(
    model=&quot;qwen-max&quot;
)
#invoke输出，一次性输出
res=model.invoke(input=&quot;What can you do? Answer simply.&quot;)
print(res)
res=model.stream(input=&quot;What can you do? Answer simply.&quot;)

for chunk in res:
    print(chunk,end=&quot;&quot;,flush=True)
</code></pre>
<ul>
<li>chat_models</li>
</ul>
<pre><code class="language-python">from langchain_community.chat_models.tongyi import ChatTongyi

#元组形式不需要该包
from langchain_core.message import HumanMessage,AIMessage,SystemMessage

models=ChatTongyi(model=&quot;qwen3-max&quot;)


#静态写法，直接得到Message类
message=[
    SystemMessage(content=&quot;你是一名边塞诗人，只输出诗句，不输出其他内容&quot;),
    HumanMessage(content=&quot;为我写一首唐诗&quot;),
    AIMessage(content=&quot;大漠孤烟直，长河落日圆。&quot;),
    HumanMessage(content=&quot;以上一首诗的格式，再写一首。&quot;)
]

#或者利用元组，动态写法，运行时LangChain将其转换为Message类
#避免导包，写法简单
#**支持变量注入**
message=[
    (&quot;system&quot;,&quot;你是一名边塞诗人，只输出诗句，不输出其他内容&quot;),
    (&quot;human&quot;,&quot;为我写一首唐诗&quot;)
    (&quot;ai&quot;,&quot;大漠孤烟直，长河落日圆。&quot;)   
    (&quot;human&quot;,&quot;以上一首诗的格式，再写一首。&quot;)

]

res=model.stream(input=message)


#聊天模型需要content
for r in res:
    print(r.content,end=&quot;&quot;,flush=True)d
</code></pre>
<ul>
<li>Prompt模板</li>
</ul>
<pre><code class="language-python">from langchain_community.embeddings import DashScopeEmbeddings


#默认text-embeddings-v1
model=DashScopeEmbeddings()


#embed_query,embed_documents
print(model.embed_query(&quot;I love you&quot;))
print(model.embed_documents([&quot;我喜欢你&quot;,&quot;I love you&quot;,&quot;holly shit&quot;]))
</code></pre>
<p><img alt="image-20260204180335027" src="/assets/image-20260204180335027.png" /></p>
<h2 id="prompt">prompt优化</h2>
<ul>
<li>通用提示词模板</li>
</ul>
<pre><code class="language-python">from langchain_core.prompts import PromptTemplate
from langchain_community.llms.tongyi import Tongyi


#这种方式生成的是Runable类型对象，可以用于chain中，字符串注入方式则不可
prompt_template=PromptTemplate.from_template(
    &quot;我的邻居姓{lastname}，刚生了一个{gender}，你帮我起个名字，简单回答。&quot;
)
#prompt_text=prompt_template.format(lastname=&quot;张&quot;,gender=&quot;女&quot;)

chain=prompt_template|model
res=chain.invoke(input=&quot;{&quot;lastname&quot;:&quot;张&quot;,&quot;gender&quot;:&quot;女&quot;}&quot;)
print(res)
</code></pre>
<ul>
<li>Fewshot提示词模板</li>
</ul>
<pre><code class="language-python">from langchain_core.prompts import FewShotPromptTemplate,PromptTemplate
from langchain_community.llms.tongyi import Tongyi 
example_template=PromptTemplate.from_template(&quot;单词：{word},反义词：{antonym}&quot;)
example_data=[
    {&quot;word&quot;:&quot;大&quot;,&quot;antonym&quot;:&quot;小&quot;},
    {&quot;word&quot;:&quot;上&quot;,&quot;antonym&quot;:&quot;下&quot;}
]

few_shot_template=FewShotTemplate(
    example_prompt=example_template,
    example=example_data,
    prefix=&quot;告知我单词的反义词，我提供如下的示例：&quot;,
    suffix=&quot;基于前面的示例告诉我，{input_word}的反义词是？&quot;,
    input_variables=['input_word']
)

prompt_text=few_shot_template.invoke(input={&quot;左&quot;}).toString()

model=Tongyi(model=&quot;qwen-max&quot;)

res=model.invoke(input=prompt_text)
print(res)
</code></pre>
<ul>
<li>format VS invoke</li>
</ul>
<p><img alt="image-20260205111156276" src="/assets/image-20260205111156276.png" /></p>
<p><img alt="image-20260205111257596" src="/assets/image-20260205111257596.png" /></p>
<pre><code class="language-python">from langchain_core.prompts import PromptTemplate

template=PromptTemplate.from_template(&quot;我的邻居是：{lastname}，最喜欢：{hobby}&quot;)
res=template.format(lastname=&quot;张晓梅&quot;,hobby=&quot;钓鱼&quot;)
print(res)

res2=template.invoke({&quot;lastname&quot;:&quot;喜刷刷&quot;,&quot;hobby&quot;:&quot;洗刷刷&quot;})
print(res2,type(res2))
</code></pre>
<ul>
<li>ChatPromptTemplate</li>
</ul>
<pre><code class="language-python">from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder
from langchain_community.chat_models.tongyi import ChatTongyi

chat_prompt_template=ChatPromptTemplate(
    [
        (&quot;system&quot;,&quot;你是一个边塞诗人。&quot;),
        MessagesPlaceholder(&quot;history&quot;),
        (&quot;human&quot;,&quot;请再写一首唐诗。&quot;),
    ]
)
history_data=[
    (&quot;human&quot;,&quot;请你写一首诗&quot;),
    (&quot;ai&quot;,&quot;征蓬出汉塞，归雁入胡天。&quot;),
    (&quot;human&quot;,&quot;请再写一首&quot;),
    (&quot;ai&quot;,&quot;大漠孤烟直，长河落日圆。&quot;)
]
prompt_text=chat_prompt_template.invoke({&quot;history&quot;:history_data}).to_string()

print(prompt_text)
model=ChatTongyi(model=&quot;qwen3-max&quot;)
res=model.invoke(prompt_text)
print(res.content)
</code></pre>
<h2 id="chain">Chain的使用</h2>
<ul>
<li>Runnable子类才可加入链</li>
</ul>
<p><img alt="image-20260205122951886" src="/assets/image-20260205122951886.png" /></p>
<ul>
<li>用invoke或stream执行，上一组件的输出作为下一组件的输出</li>
</ul>
<pre><code class="language-python">from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder
from langchain_community.chat_models.tongyi import ChatTongyi

chat_prompt_template=ChatPromptTemplate(
    [
        (&quot;system&quot;,&quot;你是一个边塞诗人。&quot;),
        MessagesPlaceholder(&quot;history&quot;),
        (&quot;human&quot;,&quot;请再写一首唐诗。&quot;),
    ]
)

history_data=[
    (&quot;human&quot;,&quot;请你写一首诗&quot;),
    (&quot;ai&quot;,&quot;征蓬出汉塞，归雁入胡天。&quot;),
    (&quot;human&quot;,&quot;请再写一首&quot;),
    (&quot;ai&quot;,&quot;大漠孤烟直，长河落日圆。&quot;)
]
model=ChatTongyi(model=&quot;qwen3-max&quot;)

chain=chat_prompt_template|model

#res=chain.invoke({&quot;history&quot;:history_data})
#print(res.content)

res=chain.stream({&quot;history&quot;:history_data})
for r in res:
    print(r.content,end=&quot;&quot;,flush=True)
</code></pre>
<ul>
<li>Runnable接口</li>
<li>上一链的输出类型需要和下一链的输入类型相同</li>
<li>model的输入：PromptValue、str、Sequence</li>
<li>StrOutputParser:字符串解析器，也为Runnable的子类，可以作为model的输入（AIMessage）</li>
<li>JsonOutputParser:Json解析器，也是Runnable的子类，可以作为Prompt的输入</li>
</ul>
<pre><code class="language-python">from langchain_core.output_parsers import StrOutputParser,JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_community.chat_models.tongyi import ChatTongyi

model = ChatTongyi(model=&quot;qwen3-max&quot;)
prompt1=PromptTemplate.from_template(
    &quot;我邻居姓{lastname}，刚生了{gender}，请起名，只输出名字,并且封装为JSON格式返回给我，要求key是name，value是名字。&quot;
)
prompt2=PromptTemplate.from_template(
    &quot;有一个名字{name}，请简单输出其含义。&quot;
)
parser=StrOutputParser()
json_parser=JsonOutputParser()
chain=prompt1|model|json_parser|prompt2|model|parser
res=chain.invoke({&quot;lastname&quot;:&quot;张&quot;,&quot;gender&quot;:&quot;女儿&quot;})
print(res)
</code></pre>
<ul>
<li>可将函数加入链-RunnableLambda-自定义函数</li>
</ul>
<pre><code class="language-python">from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.runnables import RunnableLambda

model = ChatTongyi(model=&quot;qwen3-max&quot;)

my_func=RunnableLambda(lambda ai_msg:{&quot;name&quot;:ai_msg.content})
prompt1=PromptTemplate.from_template(
    &quot;我邻居姓{lastname}，刚生了{gender}，请起名，只输出名字。&quot;
)
prompt2=PromptTemplate.from_template(
    &quot;有一个名字{name}，请简单输出其含义。&quot;
)
parser=StrOutputParser()

#也可直接传参，__or__重写，函数自动转换成Runnable
chain=prompt1|model|(lambda ai_msg:{&quot;name&quot;:ai_msg.content})|prompt2|model|parser
res=chain.invoke({&quot;lastname&quot;:&quot;张&quot;,&quot;gender&quot;:&quot;女儿&quot;})
print(res)
</code></pre>
<ul>
<li>临时会话记忆</li>
</ul>
<pre><code class="language-python">from langchain_community.chat_models import ChatTongyi
from langchain_core.prompts import ChatPromptTemplate,MessagePlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableWithMessageHistroy
from langchain_core.chat_history import InMemoryChatMessageHistory

model=ChatTongyi(model=&quot;qwen3-max&quot;)
prompt=ChatPromptTemplate(
    [
        (&quot;system&quot;,&quot;你需要根据历史会话回应用户问题。&quot;),
        MessagePlaceholder(&quot;chat_history&quot;),
        (&quot;human&quot;,&quot;请回答如下问题：{input}&quot;)
    ]
)
str_parser=StrOutputParser()

def print_prompt(full_prompt):
    print(&quot;=&quot;*20,full_prompt.to_string(),&quot;=&quot;*20)
    return full_prompt

base_chain=prompt|print_prompt|model|str_parser

store={}
def get_history(session_id)：
    if session_id not in stroe:
        store[session_id]=InMemoryChatMessageHistory()
    return store[session_id]



conversation_chain=RunnableWithMEssageHistory(
    base_chain,
    get_history,
    input_message_key=&quot;input&quot;,
    history_message_key=&quot;chat_history&quot;
)


if __name__=&quot;__main__&quot;:
    session_config={
        &quot;configruable&quot;:{
            &quot;session_id&quot;:&quot;user_001&quot;
        }
    }

    res=conversation_chain.invoke({&quot;input&quot;:&quot;小明有两只猫&quot;},session_config)
    print(&quot;第一次执行:&quot;,res)
    res=conversation_chain.invoke({&quot;input&quot;:&quot;小黄有两只狗&quot;},session_config)
    print(&quot;第二次执行&quot;,res)
    res=conversation_chain.invoke({&quot;input&quot;:&quot;他们一共几只宠物&quot;},session_config)
    print(&quot;第三次执行&quot;,res)
</code></pre>
<ul>
<li>长期会话记忆</li>
</ul>
<pre><code class="language-python">import os,json
from typing import Sequence

from langchain_community.chat_models import ChatTongyi
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import message_to_dict,messages_from_dict,BaseMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableWithMessageHistory


class FileChatMessageHistory(BaseChatMessageHistory):
    def __init__(self,session_id,storage_path):
        self.session_id=session_id
        self.storage_path=storage_path
        self.file_path=os.path.join(self.storage_path,self.session_id)

        os.makedirs(os.path.dirname(self.file_path),exist_ok=True)

    def add_messages(self,messages:Sequence[BaseMessage])-&gt;None:
        all_messages=list(self.messages)
        all_messages.extend(messages)

        new_messages=[]
        for message in all_messages:
            d=message_to_dict(message)
            new_messages.append(d)

        with open(self.file_path,'w',encoding=&quot;utf-8&quot;) as f:
            json.dump(new_messages,f)


    #将方法伪装成属性
    @property 
    def messages(self)-&gt;list[BaseMessage]:

        try:
            with open(self.file_path,'r',encoding=&quot;utf-8&quot;) as f:
                message_data=json.load(f)
                return messages_from_dict(message_data)
        except FileNotFoundError:
            return []


    def clear(self)-&gt;None:
        with open(self.file_path,'w',encoding=&quot;utf-8&quot;) as f:
            json.dump([],f)





model=ChatTongyi(model=&quot;qwen3-max&quot;)
# prompt=PromptTemplate.from_template(
#     &quot;你需要根据历史会话回应用户问题。历史对话：{chat_history},用户提问：{input},请回答。&quot;
# )


prompt=ChatPromptTemplate.from_messages(
    [
        (&quot;system&quot;,&quot;你需要根据历史会话回应用户问题。&quot;),
        MessagesPlaceholder(&quot;chat_history&quot;),
        (&quot;human&quot;,&quot;请回答如下问题：{input}&quot;)
    ]
)
str_parser=StrOutputParser()



def print_prompt(full_prompt):
    print(&quot;=&quot;*20,full_prompt.to_string(),&quot;=&quot;*20)
    return full_prompt



def get_history(session_id):
    return FileChatMessageHistory(session_id,&quot;./chat_history&quot;)

base_chain=prompt|print_prompt|model|str_parser

conversation_chain=RunnableWithMessageHistory(
    base_chain,
    get_history,
    input_messages_key=&quot;input&quot;,
    history_messages_key=&quot;chat_history&quot;
)

if __name__==&quot;__main__&quot;:
    session_config={
        &quot;configurable&quot;:{
            &quot;session_id&quot;:&quot;user_001&quot;
        }
    }

    # res=conversation_chain.invoke({&quot;input&quot;:&quot;小明有两只猫&quot;},session_config)
    # print(&quot;第一次执行:&quot;,res)
    # res=conversation_chain.invoke({&quot;input&quot;:&quot;小黄有两只狗&quot;},session_config)
    # print(&quot;第二次执行&quot;,res)
    res=conversation_chain.invoke({&quot;input&quot;:&quot;他们一共几只宠物&quot;},session_config)
    print(&quot;第三次执行&quot;,res)
</code></pre>
<h2 id="document-loaders">Document loaders文档加载</h2>
<ul>
<li>load():一次性加载</li>
<li>lazy_load()：流式加载</li>
<li>
<p>CSVLoader、JSONLoader、PDFLoader</p>
</li>
<li>
<p>CSVLoader</p>
</li>
</ul>
<pre><code class="language-python">from langchain_community.document_loaders import CSVLoader

loader = CSVLoader(
    file_path=&quot;./data/stu.csv&quot;,
    csv_args={
        &quot;delimiter&quot;: &quot;,&quot;, #指定分隔符
        &quot;quotechar&quot;:'&quot;', #指定分隔符文本的引号包围是单引号还是双引号
        # &quot;fieldnames&quot;:['a','b','c'] 表头名字，有表头则不用    
    },
    encoding=&quot;utf-8&quot;
)

#document=loader.load()
#print(document)

#批量加载
# documents = loader.load()
# for doc in documents:
#     print(doc)

#懒加载
for doc in loader.lazy_load():
    print(doc)
</code></pre>
<ul>
<li>
<p>JSONLoader</p>
</li>
<li>
<p><code>jq_schema</code></p>
<ul>
<li>.表示整个JSON对象</li>
<li>[]表示数组</li>
<li>.name表示抽取名字</li>
<li>.hobby[1]表示抽取"跳"</li>
<li>.other.addr表示抽取地址深圳</li>
<li>.[].name列表json </li>
<li>json_lines=True</li>
</ul>
</li>
<li>
<p><code>text_content=False #告知JSONLoader 抽取内容不是字符串</code></p>
</li>
<li></li>
</ul>
<p><img alt="image-20260207211843797" src="/assets/image-20260207211843797.png" /></p>
<ul>
<li>
<p>TextLoader</p>
</li>
<li>
<p><code>RecursiveCharacterTexSplitter</code>:文本分割器</p>
</li>
</ul>
<pre><code class="language-python">from langchain_community.documnet_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTexSplitter

loader=TextLoader(
    ./data/python.txt,
    encoding=&quot;utf-8&quot;
)

docs=loader.load()

splitter=RecursiveCharacterTextSplitter(
    chunk_size=500, #分段最大字符数
    chunk_overlap=50, #分段见允许重叠字符数
    sparators=[&quot;\n\n&quot;,&quot;\n&quot;,&quot;。&quot;,&quot;！&quot;,&quot;？&quot;,&quot;.&quot;,&quot;!&quot;,&quot;?&quot;,&quot; &quot;],
    length_function=len,
)
split_docs=splitter.split_documents(docs)
print(len(split_docs))
for doc in split_docs:
    print(doc)
</code></pre>
<ul>
<li>PyPDFLoader</li>
</ul>
<pre><code class="language-python">from langchain_community.document_loaders import PyPDFLoader

loader=PyPDFLoader(
    file_path=&quot;./data/saa.pdf&quot;,
    mode=&quot;single&quot;#不管有多少页，只返回一个document对象，默认page模式
    password=&quot;123&quot;
)
doc=loader.load()
print(doc)
</code></pre>
<h2 id="vector-stores">Vector stores向量存储</h2>
<pre><code class="language-python">from langchain_core.vectorstores import InMemoryVectorStore
from  langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_chroma import Chroma

#内存存储
# vector_store = InMemoryVectorStore(
#     embedding=DashScopeEmbeddings()
# )

vector_store=Chroma(
    collection_name=&quot;test&quot;,
    embedding_function=DashScopeEmbeddings(),
    persist_directory=&quot;./chroma_db&quot;
)

loader=CSVLoader(
    file_path=&quot;./data/stu.csv&quot;,
    encoding=&quot;utf-8&quot;,
    source_column=&quot;name&quot;
)
doc=loader.load()
# print(doc)

vector_store.add_documents(
    documents=doc,
    ids=[&quot;id&quot;+str(i) for i in range(1,len(doc)+1)],
)
vector_store.delete(&quot;id1&quot;)
result=vector_store.similarity_search(
    &quot;17&quot;,
    1,
    filter={&quot;name&quot;:&quot;张三&quot;}
)
print(result)
</code></pre>
<h2 id="_1">基于向量检索构建提示词</h2>
<pre><code class="language-python">from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.prompts import ChatPromptTemplate
from  langchain_core.output_parsers import StrOutputParser

model = ChatTongyi(model=&quot;qwen3-max&quot;)
prompt = ChatPromptTemplate(
    [
        (&quot;system&quot;,&quot;以我提供的资料为主，提供简单专业的回答，参考资料:{context}.&quot;),
        (&quot;user&quot;,&quot;用户提问：{input}&quot;)
    ]
)
vector_store = InMemoryVectorStore(
    embedding=DashScopeEmbeddings(model=&quot;text-embedding-v4&quot;)
)

vector_store.add_texts([&quot;减肥就要少吃多练&quot;,&quot;减肥要加强力量训练&quot;,&quot;跑步是个很好的运动&quot;])

input_text=&quot;怎么减肥？&quot;

result=vector_store.similarity_search(input_text,2)
reference_text=&quot;[&quot;
for doc in result:
    reference_text+=doc.page_content
reference_text+=&quot;]&quot;

def print_prompt(prompt):
    print(prompt.to_string())
    print(&quot;=&quot;*20)
    return prompt

chain=prompt|print_prompt|model|StrOutputParser()

res=chain.invoke({&quot;input&quot;: input_text,&quot;context&quot;: reference_text})
print(res)
</code></pre>
<ul>
<li>RunnablePassthrough</li>
<li>将向量检索加入链</li>
<li>as_retriever</li>
</ul>
<pre><code class="language-PY">from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.prompts import ChatPromptTemplate
from  langchain_core.output_parsers import StrOutputParser

model = ChatTongyi(model=&quot;qwen3-max&quot;)
prompt = ChatPromptTemplate(
    [
        (&quot;system&quot;,&quot;以我提供的资料为主，提供简单专业的回答，参考资料:{context}.&quot;),
        (&quot;user&quot;,&quot;用户提问：{input}&quot;)
    ]
)
vector_store = InMemoryVectorStore(
    embedding=DashScopeEmbeddings(model=&quot;text-embedding-v4&quot;)
)

vector_store.add_texts([&quot;减肥就要少吃多练&quot;,&quot;减肥要加强力量训练&quot;,&quot;跑步是个很好的运动&quot;])

input_text=&quot;怎么减肥？&quot;

#输入：用户输入。输出：向量库检索
retriever=vector_store.as_retriever(search_kwargs={&quot;k&quot;:2})

def print_prompt(prompt):
    print(prompt.to_string())
    print(&quot;=&quot;*20)
    return prompt

def format_func(docs:list[Document]):
    if not docs:
        return &quot;无参考资料&quot;
    formatted_str=&quot;[&quot;
    for doc in docs:
        formatted_str+=doc.page_content
    formatted_str+=&quot;]&quot;
    return formatted_str


chain=(
    {&quot;input&quot;:RunnablePassthrough(), &quot;context&quot;:retriever|format_func}|prompt|print_prompt|model|StrOutputParser()
)

res=chain.invoke(input_text)
print(res)
</code></pre>
    </div>
</article>

        </main>
    </div>

    <footer class="footer">
        <div class="footer-content">
            <div class="footer-left">
                &copy; 2026 <a href="../index.html">smart</a>.
                 <span class="desktop-only">Hexo <span style="color: var(--primary-color);">♥</span> <span>JinJia2</span></span>
            </div>

            
        </div>
    </footer>
</body>
</html>